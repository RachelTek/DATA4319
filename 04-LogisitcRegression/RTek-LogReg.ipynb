{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rachel Tekchandani\n",
    "\n",
    "### Logisitc Regression (Binary Classification)\n",
    "---\n",
    "#### Predict whether a University applicant will be admitted based on GMAT score and GPA\n",
    "\n",
    "----\n",
    "\n",
    "### Feed Foward with a single neuron\n",
    "\n",
    "## $$ (x^1, y^1), ...,(x^N, y^N) $$\n",
    "\n",
    "### Where there are N training data points\n",
    "\n",
    "## $$ x^{(i)} = \\begin{bmatrix} \\text{student} && \\text{GMAT Score} \\\\ \\text{student} && \\text{GPA} \\end{bmatrix} $$\n",
    "\n",
    "## $$ y^{(i)} \\in \\{ 0,1 \\} $$\n",
    "\n",
    "### Where 1 indicates student has been accepted and 0 if not.\n",
    "\n",
    "<center>![neuron1](neuron1.png)</center>\n",
    "<center><img src=\"neuron1.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$ \\sum = z^{(i)} = w^Tx^{(i)} + b $$\n",
    "\n",
    "## $$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "## $$ f = \\sigma = \\sigma(z^{(i)})= \\sigma(w^Tx^{(i)} + b) = \\frac{1}{1+e^{-(w^Tx^{(i)} + b)}} = \\hat{y}^{(i)} $$\n",
    "\n",
    "\n",
    "$$  $$ \n",
    "\n",
    "$$  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip930\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip930)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#f7f3ee\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip931\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip930)\" d=\"\n",
       "M174.862 1486.45 L2352.76 1486.45 L2352.76 47.2441 L174.862 47.2441  Z\n",
       "  \" fill=\"#e2dcd4\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip932\">\n",
       "    <rect x=\"174\" y=\"47\" width=\"2179\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#cbbfaf; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  236.501,1486.45 236.501,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#cbbfaf; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  750.155,1486.45 750.155,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#cbbfaf; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1263.81,1486.45 1263.81,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#cbbfaf; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1777.46,1486.45 1777.46,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#cbbfaf; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2291.12,1486.45 2291.12,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#cbbfaf; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.862,1445.78 2352.76,1445.78 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#cbbfaf; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.862,1106.31 2352.76,1106.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#cbbfaf; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.862,766.846 2352.76,766.846 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#cbbfaf; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.862,427.38 2352.76,427.38 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#cbbfaf; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.862,87.9146 2352.76,87.9146 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.862,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.862,1486.45 174.862,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.501,1486.45 236.501,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  750.155,1486.45 750.155,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1263.81,1486.45 1263.81,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1777.46,1486.45 1777.46,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2291.12,1486.45 2291.12,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.862,1445.78 200.997,1445.78 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.862,1106.31 200.997,1106.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.862,766.846 200.997,766.846 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.862,427.38 200.997,427.38 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#cbbfaf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.862,87.9146 200.997,87.9146 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"M 0 0 M195.262 1523.09 L224.938 1523.09 L224.938 1527.03 L195.262 1527.03 L195.262 1523.09 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M230.818 1535.98 L238.457 1535.98 L238.457 1509.62 L230.146 1511.29 L230.146 1507.03 L238.41 1505.36 L243.086 1505.36 L243.086 1535.98 L250.725 1535.98 L250.725 1539.92 L230.818 1539.92 L230.818 1535.98 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M265.794 1508.44 Q262.183 1508.44 260.355 1512 Q258.549 1515.55 258.549 1522.67 Q258.549 1529.78 260.355 1533.35 Q262.183 1536.89 265.794 1536.89 Q269.429 1536.89 271.234 1533.35 Q273.063 1529.78 273.063 1522.67 Q273.063 1515.55 271.234 1512 Q269.429 1508.44 265.794 1508.44 M265.794 1504.73 Q271.605 1504.73 274.66 1509.34 Q277.739 1513.92 277.739 1522.67 Q277.739 1531.4 274.66 1536.01 Q271.605 1540.59 265.794 1540.59 Q259.984 1540.59 256.906 1536.01 Q253.85 1531.4 253.85 1522.67 Q253.85 1513.92 256.906 1509.34 Q259.984 1504.73 265.794 1504.73 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M722.308 1523.09 L751.983 1523.09 L751.983 1527.03 L722.308 1527.03 L722.308 1523.09 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M757.099 1505.36 L775.456 1505.36 L775.456 1509.3 L761.382 1509.3 L761.382 1517.77 Q762.4 1517.42 763.419 1517.26 Q764.437 1517.07 765.456 1517.07 Q771.243 1517.07 774.622 1520.24 Q778.002 1523.42 778.002 1528.83 Q778.002 1534.41 774.53 1537.51 Q771.057 1540.59 764.738 1540.59 Q762.562 1540.59 760.294 1540.22 Q758.048 1539.85 755.641 1539.11 L755.641 1534.41 Q757.724 1535.54 759.946 1536.1 Q762.169 1536.66 764.645 1536.66 Q768.65 1536.66 770.988 1534.55 Q773.326 1532.44 773.326 1528.83 Q773.326 1525.22 770.988 1523.11 Q768.65 1521.01 764.645 1521.01 Q762.77 1521.01 760.895 1521.42 Q759.044 1521.84 757.099 1522.72 L757.099 1505.36 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M1263.81 1508.44 Q1260.2 1508.44 1258.37 1512 Q1256.56 1515.55 1256.56 1522.67 Q1256.56 1529.78 1258.37 1533.35 Q1260.2 1536.89 1263.81 1536.89 Q1267.44 1536.89 1269.25 1533.35 Q1271.08 1529.78 1271.08 1522.67 Q1271.08 1515.55 1269.25 1512 Q1267.44 1508.44 1263.81 1508.44 M1263.81 1504.73 Q1269.62 1504.73 1272.67 1509.34 Q1275.75 1513.92 1275.75 1522.67 Q1275.75 1531.4 1272.67 1536.01 Q1269.62 1540.59 1263.81 1540.59 Q1258 1540.59 1254.92 1536.01 Q1251.86 1531.4 1251.86 1522.67 Q1251.86 1513.92 1254.92 1509.34 Q1258 1504.73 1263.81 1504.73 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M1767.74 1505.36 L1786.1 1505.36 L1786.1 1509.3 L1772.02 1509.3 L1772.02 1517.77 Q1773.04 1517.42 1774.06 1517.26 Q1775.08 1517.07 1776.1 1517.07 Q1781.88 1517.07 1785.26 1520.24 Q1788.64 1523.42 1788.64 1528.83 Q1788.64 1534.41 1785.17 1537.51 Q1781.7 1540.59 1775.38 1540.59 Q1773.2 1540.59 1770.94 1540.22 Q1768.69 1539.85 1766.28 1539.11 L1766.28 1534.41 Q1768.37 1535.54 1770.59 1536.1 Q1772.81 1536.66 1775.29 1536.66 Q1779.29 1536.66 1781.63 1534.55 Q1783.97 1532.44 1783.97 1528.83 Q1783.97 1525.22 1781.63 1523.11 Q1779.29 1521.01 1775.29 1521.01 Q1773.41 1521.01 1771.54 1521.42 Q1769.69 1521.84 1767.74 1522.72 L1767.74 1505.36 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M2267.99 1535.98 L2275.63 1535.98 L2275.63 1509.62 L2267.32 1511.29 L2267.32 1507.03 L2275.59 1505.36 L2280.26 1505.36 L2280.26 1535.98 L2287.9 1535.98 L2287.9 1539.92 L2267.99 1539.92 L2267.99 1535.98 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M2302.97 1508.44 Q2299.36 1508.44 2297.53 1512 Q2295.72 1515.55 2295.72 1522.67 Q2295.72 1529.78 2297.53 1533.35 Q2299.36 1536.89 2302.97 1536.89 Q2306.6 1536.89 2308.41 1533.35 Q2310.24 1529.78 2310.24 1522.67 Q2310.24 1515.55 2308.41 1512 Q2306.6 1508.44 2302.97 1508.44 M2302.97 1504.73 Q2308.78 1504.73 2311.83 1509.34 Q2314.91 1513.92 2314.91 1522.67 Q2314.91 1531.4 2311.83 1536.01 Q2308.78 1540.59 2302.97 1540.59 Q2297.16 1540.59 2294.08 1536.01 Q2291.02 1531.4 2291.02 1522.67 Q2291.02 1513.92 2294.08 1509.34 Q2297.16 1504.73 2302.97 1504.73 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M74.9365 1431.58 Q71.3254 1431.58 69.4967 1435.14 Q67.6912 1438.68 67.6912 1445.81 Q67.6912 1452.92 69.4967 1456.48 Q71.3254 1460.02 74.9365 1460.02 Q78.5707 1460.02 80.3763 1456.48 Q82.205 1452.92 82.205 1445.81 Q82.205 1438.68 80.3763 1435.14 Q78.5707 1431.58 74.9365 1431.58 M74.9365 1427.87 Q80.7467 1427.87 83.8022 1432.48 Q86.8809 1437.06 86.8809 1445.81 Q86.8809 1454.54 83.8022 1459.15 Q80.7467 1463.73 74.9365 1463.73 Q69.1264 1463.73 66.0477 1459.15 Q62.9921 1454.54 62.9921 1445.81 Q62.9921 1437.06 66.0477 1432.48 Q69.1264 1427.87 74.9365 1427.87 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M91.9503 1457.18 L96.8345 1457.18 L96.8345 1463.06 L91.9503 1463.06 L91.9503 1457.18 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M111.904 1431.58 Q108.293 1431.58 106.464 1435.14 Q104.659 1438.68 104.659 1445.81 Q104.659 1452.92 106.464 1456.48 Q108.293 1460.02 111.904 1460.02 Q115.538 1460.02 117.344 1456.48 Q119.172 1452.92 119.172 1445.81 Q119.172 1438.68 117.344 1435.14 Q115.538 1431.58 111.904 1431.58 M111.904 1427.87 Q117.714 1427.87 120.77 1432.48 Q123.848 1437.06 123.848 1445.81 Q123.848 1454.54 120.77 1459.15 Q117.714 1463.73 111.904 1463.73 Q106.094 1463.73 103.015 1459.15 Q99.9595 1454.54 99.9595 1445.81 Q99.9595 1437.06 103.015 1432.48 Q106.094 1427.87 111.904 1427.87 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M138.918 1431.58 Q135.307 1431.58 133.478 1435.14 Q131.672 1438.68 131.672 1445.81 Q131.672 1452.92 133.478 1456.48 Q135.307 1460.02 138.918 1460.02 Q142.552 1460.02 144.357 1456.48 Q146.186 1452.92 146.186 1445.81 Q146.186 1438.68 144.357 1435.14 Q142.552 1431.58 138.918 1431.58 M138.918 1427.87 Q144.728 1427.87 147.783 1432.48 Q150.862 1437.06 150.862 1445.81 Q150.862 1454.54 147.783 1459.15 Q144.728 1463.73 138.918 1463.73 Q133.107 1463.73 130.029 1459.15 Q126.973 1454.54 126.973 1445.81 Q126.973 1437.06 130.029 1432.48 Q133.107 1427.87 138.918 1427.87 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M77.5291 1092.11 Q73.918 1092.11 72.0893 1095.68 Q70.2838 1099.22 70.2838 1106.35 Q70.2838 1113.45 72.0893 1117.02 Q73.918 1120.56 77.5291 1120.56 Q81.1633 1120.56 82.9689 1117.02 Q84.7976 1113.45 84.7976 1106.35 Q84.7976 1099.22 82.9689 1095.68 Q81.1633 1092.11 77.5291 1092.11 M77.5291 1088.41 Q83.3392 1088.41 86.3948 1093.01 Q89.4735 1097.6 89.4735 1106.35 Q89.4735 1115.07 86.3948 1119.68 Q83.3392 1124.26 77.5291 1124.26 Q71.7189 1124.26 68.6402 1119.68 Q65.5847 1115.07 65.5847 1106.35 Q65.5847 1097.6 68.6402 1093.01 Q71.7189 1088.41 77.5291 1088.41 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M94.5429 1117.71 L99.4271 1117.71 L99.4271 1123.59 L94.5429 1123.59 L94.5429 1117.71 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M108.524 1119.66 L124.844 1119.66 L124.844 1123.59 L102.899 1123.59 L102.899 1119.66 Q105.561 1116.9 110.145 1112.27 Q114.751 1107.62 115.932 1106.28 Q118.177 1103.75 119.057 1102.02 Q119.959 1100.26 119.959 1098.57 Q119.959 1095.81 118.015 1094.08 Q116.094 1092.34 112.992 1092.34 Q110.793 1092.34 108.339 1093.11 Q105.909 1093.87 103.131 1095.42 L103.131 1090.7 Q105.955 1089.56 108.409 1088.99 Q110.862 1088.41 112.899 1088.41 Q118.27 1088.41 121.464 1091.09 Q124.658 1093.78 124.658 1098.27 Q124.658 1100.4 123.848 1102.32 Q123.061 1104.22 120.955 1106.81 Q120.376 1107.48 117.274 1110.7 Q114.172 1113.89 108.524 1119.66 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M129.959 1089.03 L148.316 1089.03 L148.316 1092.97 L134.242 1092.97 L134.242 1101.44 Q135.26 1101.09 136.279 1100.93 Q137.297 1100.74 138.316 1100.74 Q144.103 1100.74 147.482 1103.92 Q150.862 1107.09 150.862 1112.5 Q150.862 1118.08 147.39 1121.18 Q143.918 1124.26 137.598 1124.26 Q135.422 1124.26 133.154 1123.89 Q130.908 1123.52 128.501 1122.78 L128.501 1118.08 Q130.584 1119.22 132.807 1119.77 Q135.029 1120.33 137.506 1120.33 Q141.51 1120.33 143.848 1118.22 Q146.186 1116.11 146.186 1112.5 Q146.186 1108.89 143.848 1106.79 Q141.51 1104.68 137.506 1104.68 Q135.631 1104.68 133.756 1105.1 Q131.904 1105.51 129.959 1106.39 L129.959 1089.03 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M75.9319 752.645 Q72.3208 752.645 70.4921 756.209 Q68.6865 759.751 68.6865 766.881 Q68.6865 773.987 70.4921 777.552 Q72.3208 781.094 75.9319 781.094 Q79.5661 781.094 81.3717 777.552 Q83.2004 773.987 83.2004 766.881 Q83.2004 759.751 81.3717 756.209 Q79.5661 752.645 75.9319 752.645 M75.9319 748.941 Q81.742 748.941 84.7976 753.547 Q87.8763 758.131 87.8763 766.881 Q87.8763 775.608 84.7976 780.214 Q81.742 784.797 75.9319 784.797 Q70.1217 784.797 67.043 780.214 Q63.9875 775.608 63.9875 766.881 Q63.9875 758.131 67.043 753.547 Q70.1217 748.941 75.9319 748.941 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M92.9457 778.246 L97.8299 778.246 L97.8299 784.126 L92.9457 784.126 L92.9457 778.246 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M102.946 749.566 L121.302 749.566 L121.302 753.501 L107.228 753.501 L107.228 761.973 Q108.247 761.626 109.265 761.464 Q110.284 761.279 111.302 761.279 Q117.089 761.279 120.469 764.45 Q123.848 767.621 123.848 773.038 Q123.848 778.617 120.376 781.719 Q116.904 784.797 110.584 784.797 Q108.409 784.797 106.14 784.427 Q103.895 784.057 101.487 783.316 L101.487 778.617 Q103.571 779.751 105.793 780.307 Q108.015 780.862 110.492 780.862 Q114.496 780.862 116.834 778.756 Q119.172 776.649 119.172 773.038 Q119.172 769.427 116.834 767.321 Q114.496 765.214 110.492 765.214 Q108.617 765.214 106.742 765.631 Q104.89 766.047 102.946 766.927 L102.946 749.566 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M138.918 752.645 Q135.307 752.645 133.478 756.209 Q131.672 759.751 131.672 766.881 Q131.672 773.987 133.478 777.552 Q135.307 781.094 138.918 781.094 Q142.552 781.094 144.357 777.552 Q146.186 773.987 146.186 766.881 Q146.186 759.751 144.357 756.209 Q142.552 752.645 138.918 752.645 M138.918 748.941 Q144.728 748.941 147.783 753.547 Q150.862 758.131 150.862 766.881 Q150.862 775.608 147.783 780.214 Q144.728 784.797 138.918 784.797 Q133.107 784.797 130.029 780.214 Q126.973 775.608 126.973 766.881 Q126.973 758.131 130.029 753.547 Q133.107 748.941 138.918 748.941 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M76.8346 413.179 Q73.2236 413.179 71.3949 416.744 Q69.5893 420.285 69.5893 427.415 Q69.5893 434.521 71.3949 438.086 Q73.2236 441.628 76.8346 441.628 Q80.4689 441.628 82.2744 438.086 Q84.1031 434.521 84.1031 427.415 Q84.1031 420.285 82.2744 416.744 Q80.4689 413.179 76.8346 413.179 M76.8346 409.475 Q82.6448 409.475 85.7003 414.082 Q88.779 418.665 88.779 427.415 Q88.779 436.142 85.7003 440.748 Q82.6448 445.332 76.8346 445.332 Q71.0245 445.332 67.9458 440.748 Q64.8903 436.142 64.8903 427.415 Q64.8903 418.665 67.9458 414.082 Q71.0245 409.475 76.8346 409.475 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M93.8484 438.781 L98.7327 438.781 L98.7327 444.66 L93.8484 444.66 L93.8484 438.781 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M102.622 410.1 L124.844 410.1 L124.844 412.091 L112.297 444.66 L107.413 444.66 L119.219 414.035 L102.622 414.035 L102.622 410.1 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M129.959 410.1 L148.316 410.1 L148.316 414.035 L134.242 414.035 L134.242 422.508 Q135.26 422.16 136.279 421.998 Q137.297 421.813 138.316 421.813 Q144.103 421.813 147.482 424.984 Q150.862 428.156 150.862 433.572 Q150.862 439.151 147.39 442.253 Q143.918 445.332 137.598 445.332 Q135.422 445.332 133.154 444.961 Q130.908 444.591 128.501 443.85 L128.501 439.151 Q130.584 440.285 132.807 440.841 Q135.029 441.396 137.506 441.396 Q141.51 441.396 143.848 439.29 Q146.186 437.183 146.186 433.572 Q146.186 429.961 143.848 427.855 Q141.51 425.748 137.506 425.748 Q135.631 425.748 133.756 426.165 Q131.904 426.582 129.959 427.461 L129.959 410.1 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M66.9736 101.259 L74.6124 101.259 L74.6124 74.8939 L66.3023 76.5605 L66.3023 72.3013 L74.5661 70.6346 L79.242 70.6346 L79.242 101.259 L86.8809 101.259 L86.8809 105.195 L66.9736 105.195 L66.9736 101.259 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M91.9503 99.315 L96.8345 99.315 L96.8345 105.195 L91.9503 105.195 L91.9503 99.315 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M111.904 73.7133 Q108.293 73.7133 106.464 77.2781 Q104.659 80.8198 104.659 87.9494 Q104.659 95.0558 106.464 98.6206 Q108.293 102.162 111.904 102.162 Q115.538 102.162 117.344 98.6206 Q119.172 95.0558 119.172 87.9494 Q119.172 80.8198 117.344 77.2781 Q115.538 73.7133 111.904 73.7133 M111.904 70.0096 Q117.714 70.0096 120.77 74.6161 Q123.848 79.1994 123.848 87.9494 Q123.848 96.6762 120.77 101.283 Q117.714 105.866 111.904 105.866 Q106.094 105.866 103.015 101.283 Q99.9595 96.6762 99.9595 87.9494 Q99.9595 79.1994 103.015 74.6161 Q106.094 70.0096 111.904 70.0096 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M138.918 73.7133 Q135.307 73.7133 133.478 77.2781 Q131.672 80.8198 131.672 87.9494 Q131.672 95.0558 133.478 98.6206 Q135.307 102.162 138.918 102.162 Q142.552 102.162 144.357 98.6206 Q146.186 95.0558 146.186 87.9494 Q146.186 80.8198 144.357 77.2781 Q142.552 73.7133 138.918 73.7133 M138.918 70.0096 Q144.728 70.0096 147.783 74.6161 Q150.862 79.1994 150.862 87.9494 Q150.862 96.6762 147.783 101.283 Q144.728 105.866 138.918 105.866 Q133.107 105.866 130.029 101.283 Q126.973 96.6762 126.973 87.9494 Q126.973 79.1994 130.029 74.6161 Q133.107 70.0096 138.918 70.0096 Z\" fill=\"#725b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip932)\" style=\"stroke:#6494ed; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.501,1445.72 256.566,1445.7 442.096,1445.32 553.591,1444.43 647.86,1442.41 750.262,1436.68 844.667,1423.2 895.273,1409.22 945.879,1386.95 975.196,1368.63 \n",
       "  1004.51,1345.04 1033.83,1314.97 1063.14,1277.14 1089.2,1235.98 1115.25,1186.97 1141.3,1129.65 1167.36,1064.05 1213.05,931.253 1258.74,783.596 1308.56,621.28 \n",
       "  1358.37,474.713 1383.43,410.91 1408.48,354.739 1433.54,306.276 1458.6,265.185 1486.3,227.595 1514,197.238 1541.71,173.017 1569.41,153.879 1622.13,128.186 \n",
       "  1674.84,112.312 1784.47,96.4072 1876.75,91.3862 1989.42,89.076 2095.33,88.3291 2265,87.9941 2291.12,87.9763 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"\n",
       "M1987.15 216.178 L2280.16 216.178 L2280.16 95.2176 L1987.15 95.2176  Z\n",
       "  \" fill=\"#f7f3ee\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#725b61; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1987.15,216.178 2280.16,216.178 2280.16,95.2176 1987.15,95.2176 1987.15,216.178 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#6494ed; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2011.35,155.698 2156.54,155.698 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"M 0 0 M2194.59 175.385 Q2192.78 180.015 2191.07 181.427 Q2189.35 182.839 2186.48 182.839 L2183.08 182.839 L2183.08 179.274 L2185.58 179.274 Q2187.34 179.274 2188.31 178.44 Q2189.29 177.607 2190.47 174.505 L2191.23 172.561 L2180.74 147.052 L2185.26 147.052 L2193.36 167.329 L2201.46 147.052 L2205.97 147.052 L2194.59 175.385 Z\" fill=\"#cbbfaf\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M2211.85 169.042 L2219.49 169.042 L2219.49 142.677 L2211.18 144.343 L2211.18 140.084 L2219.45 138.418 L2224.12 138.418 L2224.12 169.042 L2231.76 169.042 L2231.76 172.978 L2211.85 172.978 L2211.85 169.042 Z\" fill=\"#cbbfaf\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "theme(:sand)\n",
    "σ(x) = 1/(1+exp(-x))\n",
    "plot(σ,-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sigma or sigmoid function, is our S shaped activation function that the output of the linear combination of the weights and biases is entered into.  This gives us a value between 0 and 1 that can be interpretted as a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Function (cross-entropy)\n",
    "\n",
    "---\n",
    "\n",
    "#### Create a function that maximizing the probability that the $\\hat{y}^{(i)}$ predicts $y^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$ \\text{Prediction} = \\begin{Bmatrix}1 && \\text{if} \\ \\ \\hat{y}^{(i)} \\geq 0.5 \\\\ 0 && \\text{otherwise}\\end{Bmatrix} $$\n",
    "\n",
    "### Since there are only two discrete outputs, this is subject to the following formula by Bernoulli:\n",
    "\n",
    "## $$ P(y^{(i)} | x^{(i)}) = \\hat{y}^y (1-\\hat{y})^{(1-y)} $$\n",
    "\n",
    "### Take the log of both sides \n",
    "\n",
    "### $$ \\text{log} P(y|x) = \\text{log}[\\hat{y}^y (1-\\hat{y})^{(1-y)}] $$\n",
    "\n",
    "### Using the log rules\n",
    "\n",
    "### $$ \\text{log} P(y|x)= y\\text{log}\\hat{y} + (1-y)\\text{log}(1-\\hat{y}) $$\n",
    "\n",
    "### To maximize the probabilty, minimize the negative\n",
    "\n",
    "### $$ -\\text{log} P(y|x)= -[y\\text{log}\\hat{y} + (1-y)\\text{log}(1-\\hat{y})] $$\n",
    "\n",
    "### Minimize the negative log or the  **Cross-Entropy loss function**.  $L_{CE}(\\hat{y},y) = -\\text{log}P(y|x) $.\n",
    "\n",
    "### replace $y$ and $\\hat{y}$ with the activation function\n",
    "\n",
    "### $$ -\\text{log} P(y|x)= -[y \\ \\text{log}\\sigma(z)+ (1-y)\\text{log}(1-\\sigma(z))] $$\n",
    "\n",
    "### $$ -\\text{log} P(y|x)= -[y \\ \\text{log}\\sigma(w^Tx^{(i)} + b)+ (1-y)\\text{log}(1-\\sigma(w^Tx^{(i)} + b))] $$\n",
    "\n",
    "### $$ = L_{CE}^{(w,b)}$$\n",
    "\n",
    "### Minmize the loss function with paramaters w and b.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train (Stochastic Gradient Descent)\n",
    "---\n",
    "\n",
    "### We Want $\\hat{\\theta} = \\text{argmin} L_{\\text{CE}}(x,y,\\theta )$, where $\\theta = w,b$\n",
    "\n",
    "### $$ \\frac{\\partial L_{\\text{CE}}} {\\partial w_j} (w,b) = [ \\sigma ( w^T x + b ) - y ] x_j $$\n",
    "\n",
    "### $$ \\frac{\\partial L_{\\text{CE}}} {\\partial b} (w,b) = \\sigma ( w^T x + b ) - y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a given $(x^{(i)}, y^{(i)})$\n",
    "### $$ w_j^{k+1} = w_j^k - \\alpha \\cdot \\frac{\\partial L_{\\text{CE}}} {\\partial w^k_j} (w^k,b^k) $$\n",
    "### (for j= 1,2)\n",
    "### $$ b^{k+1} = b^k - \\alpha \\cdot \\frac{\\partial L_{\\text{CE}}} {\\partial b} (w^k,b^k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## We will use our binary classifier on data that shows whether a University Candidate was accepted with their GPA, GMAT and Work expereience.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>gmat</th><th>gpa</th><th>work_experience</th><th>admitted</th></tr><tr><th></th><th>Int64</th><th>Float64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>40 rows × 4 columns</p><tr><th>1</th><td>780</td><td>4.0</td><td>3</td><td>1</td></tr><tr><th>2</th><td>750</td><td>3.9</td><td>4</td><td>1</td></tr><tr><th>3</th><td>690</td><td>3.3</td><td>3</td><td>0</td></tr><tr><th>4</th><td>710</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>5</th><td>680</td><td>3.9</td><td>4</td><td>0</td></tr><tr><th>6</th><td>730</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>7</th><td>690</td><td>2.3</td><td>1</td><td>0</td></tr><tr><th>8</th><td>720</td><td>3.3</td><td>4</td><td>1</td></tr><tr><th>9</th><td>740</td><td>3.3</td><td>5</td><td>1</td></tr><tr><th>10</th><td>690</td><td>1.7</td><td>1</td><td>0</td></tr><tr><th>11</th><td>610</td><td>2.7</td><td>3</td><td>0</td></tr><tr><th>12</th><td>690</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>13</th><td>710</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>14</th><td>680</td><td>3.3</td><td>4</td><td>0</td></tr><tr><th>15</th><td>770</td><td>3.3</td><td>3</td><td>1</td></tr><tr><th>16</th><td>610</td><td>3.0</td><td>1</td><td>0</td></tr><tr><th>17</th><td>580</td><td>2.7</td><td>4</td><td>0</td></tr><tr><th>18</th><td>650</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>19</th><td>540</td><td>2.7</td><td>2</td><td>0</td></tr><tr><th>20</th><td>590</td><td>2.3</td><td>3</td><td>0</td></tr><tr><th>21</th><td>620</td><td>3.3</td><td>2</td><td>1</td></tr><tr><th>22</th><td>600</td><td>2.0</td><td>1</td><td>0</td></tr><tr><th>23</th><td>550</td><td>2.3</td><td>4</td><td>0</td></tr><tr><th>24</th><td>550</td><td>2.7</td><td>1</td><td>0</td></tr><tr><th>25</th><td>570</td><td>3.0</td><td>2</td><td>0</td></tr><tr><th>26</th><td>670</td><td>3.3</td><td>6</td><td>1</td></tr><tr><th>27</th><td>660</td><td>3.7</td><td>4</td><td>1</td></tr><tr><th>28</th><td>580</td><td>2.3</td><td>2</td><td>0</td></tr><tr><th>29</th><td>650</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>30</th><td>660</td><td>3.3</td><td>5</td><td>1</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& gmat & gpa & work\\_experience & admitted\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 780 & 4.0 & 3 & 1 \\\\\n",
       "\t2 & 750 & 3.9 & 4 & 1 \\\\\n",
       "\t3 & 690 & 3.3 & 3 & 0 \\\\\n",
       "\t4 & 710 & 3.7 & 5 & 1 \\\\\n",
       "\t5 & 680 & 3.9 & 4 & 0 \\\\\n",
       "\t6 & 730 & 3.7 & 6 & 1 \\\\\n",
       "\t7 & 690 & 2.3 & 1 & 0 \\\\\n",
       "\t8 & 720 & 3.3 & 4 & 1 \\\\\n",
       "\t9 & 740 & 3.3 & 5 & 1 \\\\\n",
       "\t10 & 690 & 1.7 & 1 & 0 \\\\\n",
       "\t11 & 610 & 2.7 & 3 & 0 \\\\\n",
       "\t12 & 690 & 3.7 & 5 & 1 \\\\\n",
       "\t13 & 710 & 3.7 & 6 & 1 \\\\\n",
       "\t14 & 680 & 3.3 & 4 & 0 \\\\\n",
       "\t15 & 770 & 3.3 & 3 & 1 \\\\\n",
       "\t16 & 610 & 3.0 & 1 & 0 \\\\\n",
       "\t17 & 580 & 2.7 & 4 & 0 \\\\\n",
       "\t18 & 650 & 3.7 & 6 & 1 \\\\\n",
       "\t19 & 540 & 2.7 & 2 & 0 \\\\\n",
       "\t20 & 590 & 2.3 & 3 & 0 \\\\\n",
       "\t21 & 620 & 3.3 & 2 & 1 \\\\\n",
       "\t22 & 600 & 2.0 & 1 & 0 \\\\\n",
       "\t23 & 550 & 2.3 & 4 & 0 \\\\\n",
       "\t24 & 550 & 2.7 & 1 & 0 \\\\\n",
       "\t25 & 570 & 3.0 & 2 & 0 \\\\\n",
       "\t26 & 670 & 3.3 & 6 & 1 \\\\\n",
       "\t27 & 660 & 3.7 & 4 & 1 \\\\\n",
       "\t28 & 580 & 2.3 & 2 & 0 \\\\\n",
       "\t29 & 650 & 3.7 & 6 & 1 \\\\\n",
       "\t30 & 660 & 3.3 & 5 & 1 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "40×4 DataFrame\n",
       "│ Row │ gmat  │ gpa     │ work_experience │ admitted │\n",
       "│     │ \u001b[90mInt64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mInt64\u001b[39m           │ \u001b[90mInt64\u001b[39m    │\n",
       "├─────┼───────┼─────────┼─────────────────┼──────────┤\n",
       "│ 1   │ 780   │ 4.0     │ 3               │ 1        │\n",
       "│ 2   │ 750   │ 3.9     │ 4               │ 1        │\n",
       "│ 3   │ 690   │ 3.3     │ 3               │ 0        │\n",
       "│ 4   │ 710   │ 3.7     │ 5               │ 1        │\n",
       "│ 5   │ 680   │ 3.9     │ 4               │ 0        │\n",
       "│ 6   │ 730   │ 3.7     │ 6               │ 1        │\n",
       "│ 7   │ 690   │ 2.3     │ 1               │ 0        │\n",
       "│ 8   │ 720   │ 3.3     │ 4               │ 1        │\n",
       "│ 9   │ 740   │ 3.3     │ 5               │ 1        │\n",
       "│ 10  │ 690   │ 1.7     │ 1               │ 0        │\n",
       "⋮\n",
       "│ 30  │ 660   │ 3.3     │ 5               │ 1        │\n",
       "│ 31  │ 640   │ 3.0     │ 1               │ 0        │\n",
       "│ 32  │ 620   │ 2.7     │ 2               │ 0        │\n",
       "│ 33  │ 660   │ 4.0     │ 4               │ 1        │\n",
       "│ 34  │ 660   │ 3.3     │ 6               │ 1        │\n",
       "│ 35  │ 680   │ 3.3     │ 5               │ 1        │\n",
       "│ 36  │ 650   │ 2.3     │ 1               │ 0        │\n",
       "│ 37  │ 670   │ 2.7     │ 2               │ 0        │\n",
       "│ 38  │ 580   │ 3.3     │ 1               │ 0        │\n",
       "│ 39  │ 590   │ 1.7     │ 4               │ 0        │\n",
       "│ 40  │ 690   │ 3.7     │ 5               │ 1        │"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "data = CSV.read(\"candidates_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40-element Array{Array{Float64,1},1}:\n",
       " [780.0, 4.0, 3.0]\n",
       " [750.0, 3.9, 4.0]\n",
       " [690.0, 3.3, 3.0]\n",
       " [710.0, 3.7, 5.0]\n",
       " [680.0, 3.9, 4.0]\n",
       " [730.0, 3.7, 6.0]\n",
       " [690.0, 2.3, 1.0]\n",
       " [720.0, 3.3, 4.0]\n",
       " [740.0, 3.3, 5.0]\n",
       " [690.0, 1.7, 1.0]\n",
       " [610.0, 2.7, 3.0]\n",
       " [690.0, 3.7, 5.0]\n",
       " [710.0, 3.7, 6.0]\n",
       " ⋮\n",
       " [650.0, 3.7, 6.0]\n",
       " [660.0, 3.3, 5.0]\n",
       " [640.0, 3.0, 1.0]\n",
       " [620.0, 2.7, 2.0]\n",
       " [660.0, 4.0, 4.0]\n",
       " [660.0, 3.3, 6.0]\n",
       " [680.0, 3.3, 5.0]\n",
       " [650.0, 2.3, 1.0]\n",
       " [670.0, 2.7, 2.0]\n",
       " [580.0, 3.3, 1.0]\n",
       " [590.0, 1.7, 4.0]\n",
       " [690.0, 3.7, 5.0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = [[x[1], x[2], x[3]] for x in zip(data.gmat, data.gpa, data.work_experience)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will split the data into the first and second half.  We will use the first half for training. The second half will be for testing.  Split the data and check that the two lengths add to the total of the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_data[1:20]\n",
    "x_test = x_data[21:40]\n",
    "length(x_train) + length(x_test) == length(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40-element Array{Int64,1}:\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = [x for x in data.admitted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_data[1:20]\n",
    "y_test  = y_data[21:40]\n",
    "length(y_train)+length(y_test) == length(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    σ(x)\n",
    "\n",
    "Compute the sigma (σ) of x which is defined mathematically as 1 / (1+e^(-x)\n",
    "\n",
    "input   : x the value to be computed\n",
    "\n",
    "returns : the value of the calculation\n",
    "```\n",
    "\"\"\"\n",
    "σ(x) = 1/(1+exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_entropy_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    cross_entropy_loss(x, y, w, b)\n",
    "\n",
    "Compute the loss function\n",
    "\n",
    "input   : x - feature vector (the GPA, GMAT and work experience of the candidate)\n",
    "input   : y - correct classification (Accepted or not)\n",
    "input   : w - weights for each of the values of x going into the logistic function\n",
    "input   : b - the bias or threshold value\n",
    "\n",
    "\n",
    "returns : the cross entropy loss calculation given the above as inputs\n",
    "```\n",
    "\"\"\"\n",
    "#create loss entropy loss function\n",
    "function cross_entropy_loss(x, y, w, b)\n",
    "    return -y * log(σ(w'x+ b)) - (1-y)*log(1 - σ(w'x+b))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_cost"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    average_cost(features, labels, w, b)\n",
    "\n",
    "Compute the loss function\n",
    "\n",
    "input   : features - feature vector (the GPA, GMAT and work experience of the candidate)\n",
    "input   : labels - correct classification (Accepted or not)\n",
    "input   : w - weights for each of the values of x going into the logistic function\n",
    "input   : b - the bias or threshold value\n",
    "\n",
    "\n",
    "returns : the amount of error observed for the amount of data used\n",
    "```\n",
    "\"\"\"\n",
    "function average_cost(features, labels, w, b)\n",
    "    N = length(features)\n",
    "    return (1/N)*sum([cross_entropy_loss(features[i], labels[i], w, b) for i = 1:N])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    function batch_gradient_descent(features, labels, w, b, α)\n",
    "\n",
    "Performs gradient descent on the data\n",
    "\n",
    "input   : features - feature vector (the GPA, GMAT and work experience of the candidate)\n",
    "input   : labels - correct classification (Accepted or not)\n",
    "input   : w - initial weights for each of the values of x going into the logistic function\n",
    "input   : b - intial bias \n",
    "input   : α - learning rate, the amount of change to the weights and bias\n",
    "\n",
    "returns : trained weights and bias after a step has been performed\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "function batch_gradient_descent(features, labels, w, b, α)\n",
    "    del_w = [0.0 for i = 1:length(w)]\n",
    "    del_b = 0.0\n",
    "    N = length(features)\n",
    "    \n",
    "    for i = 1:N\n",
    "        del_w += (σ(w'features[i]+b) - labels[i])*features[i]\n",
    "        del_b += (σ(w'features[i]+b) - labels[i])\n",
    "    end\n",
    "    \n",
    "    w = w - α*del_w\n",
    "    b = b - α*del_b\n",
    "    \n",
    "    return w, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.044500000000000005, 0.0004200000000000001, 0.0010500000000000002], 0.0)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test function with 0.0,0.0 weights initial, 0.0 bias and 0.0001 step size\n",
    "w, b = batch_gradient_descent(x_train, y_train, [0.0, 0.0, 0.0], 0.0, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial cost is: 0.6931471805599453\n",
      "The new cost is: 1.464413816269031\n",
      "The new cost is: 19.1459615815847\n"
     ]
    }
   ],
   "source": [
    "# check to see, based on cost that we are going in the right direction\n",
    "w = [0.0, 0.0, 0.0]\n",
    "b = 0.0\n",
    "step = 0.00001\n",
    "println(\"The initial cost is: \", average_cost(x_train, y_train, w, b))\n",
    "\n",
    "\n",
    "w, b = batch_gradient_descent(x_train, y_train, w, b, step)\n",
    "println(\"The new cost is: \", average_cost(x_train, y_train, w, b))\n",
    "\n",
    "w, b = batch_gradient_descent(x_train, y_train, w, b, step)\n",
    "println(\"The new cost is: \", average_cost(x_train, y_train, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial cost is: 0.6931471805599451\n",
      "The new cost is: 0.6931468088570281\n",
      "The new cost is: 0.6931464402540243\n"
     ]
    }
   ],
   "source": [
    "# the above results indicate that the step size is too large.  \n",
    "# We need to repeat with smaller step size\n",
    "\n",
    "w = [0.0, 0.0, 0.0]\n",
    "b = 0.0\n",
    "step = 0.000000001\n",
    "println(\"The initial cost is: \", average_cost(x_data, y_data, w, b))\n",
    "\n",
    "\n",
    "w, b = batch_gradient_descent(x_data, y_data, w, b, step)\n",
    "println(\"The new cost is: \", average_cost(x_data, y_data, w, b))\n",
    "\n",
    "w, b = batch_gradient_descent(x_data, y_data, w, b, step)\n",
    "println(\"The new cost is: \", average_cost(x_data, y_data, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial cost is: 0.6931471805599451\n",
      "The new cost is: 0.6931177157407156\n",
      "The new cost is: 0.6931074013001591\n",
      "The new cost is: 0.6931032778951178\n",
      "The new cost is: 0.6931011560327422\n"
     ]
    }
   ],
   "source": [
    "# the above output indicates that the step is too small\n",
    "# repeat with a larger and ensure that it is getting smaller\n",
    "w = [0.0, 0.0, 0.0]\n",
    "b = 0.0\n",
    "step = 0.0000001\n",
    "println(\"The initial cost is: \", average_cost(x_data, y_data, w, b))\n",
    "\n",
    "\n",
    "w, b = batch_gradient_descent(x_data, y_data, w, b, step)\n",
    "println(\"The new cost is: \", average_cost(x_data, y_data, w, b))\n",
    "\n",
    "w, b = batch_gradient_descent(x_data, y_data, w, b, step)\n",
    "println(\"The new cost is: \", average_cost(x_data, y_data, w, b))\n",
    "\n",
    "w, b = batch_gradient_descent(x_data, y_data, w, b, step)\n",
    "println(\"The new cost is: \", average_cost(x_data, y_data, w, b))\n",
    "\n",
    "w, b = batch_gradient_descent(x_data, y_data, w, b, step)\n",
    "println(\"The new cost is: \", average_cost(x_data, y_data, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    train_batch_gradient_descent(features, labels, w, b, α, epochs)\n",
    "\n",
    "Performs gradient descent on the data\n",
    "\n",
    "input   : features - feature vector (the GPA, GMAT and work experience of the candidate)\n",
    "input   : labels - correct classification (Accepted or not)\n",
    "input   : w - initial weights for each of the values of x going into the logistic function\n",
    "input   : b - intial bias \n",
    "input   : α - learning rate, the amount of change to the weights and bias\n",
    "input   : epochs - how many times to run the update\n",
    "\n",
    "returns : trained weights and bias after a step has been performed.\n",
    "          also prints the cost function at 100,1000,10000, etc epochs\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "function train_batch_gradient_descent(features, labels, w, b, α, epochs)\n",
    "    \n",
    "    for i = 1:epochs\n",
    "        j = rand([x for x in length(features)])\n",
    "        w,b = batch_gradient_descent(features, labels, w, b, α)\n",
    "        \n",
    "        if i == 1\n",
    "            println(\"Epoch \", i, \" with loss: \", average_cost(features, labels, w, b) )\n",
    "        end\n",
    "        \n",
    "        if i == 100\n",
    "            println(\"Epoch \", i, \" with loss: \", average_cost(features, labels, w, b) )\n",
    "        end\n",
    "        \n",
    "        if i == 1000\n",
    "            println(\"Epoch \", i, \" with loss: \", average_cost(features, labels, w, b) )\n",
    "        end\n",
    "\n",
    "        if i == 10000\n",
    "            println(\"Epoch \", i, \" with loss: \", average_cost(features, labels, w, b) )\n",
    "        end\n",
    "                        \n",
    "        if i == 100000\n",
    "            println(\"Epoch \", i, \" with loss: \", average_cost(features, labels, w, b) )\n",
    "        end\n",
    "                        \n",
    "        if i == 1000000\n",
    "            println(\"Epoch \", i, \" with loss: \", average_cost(features, labels, w, b) )\n",
    "        end\n",
    "\n",
    "         if i == 10000000\n",
    "            println(\"Epoch \", i, \" with loss: \", average_cost(features, labels, w, b) )\n",
    "         end\n",
    "    end\n",
    "    \n",
    "    return w, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: NaN\n",
      "Epoch 100 with loss: NaN\n",
      "Epoch 1000 with loss: NaN\n",
      "Epoch 10000 with loss: 0.7088820618671335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0003556207243116096, 1.78671706219673, -0.5852586008155235], -3.220108727980861)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = randn(3) # initlaze with random weights\n",
    "b = randn(1)[1] # random bias\n",
    "\n",
    "w, b = train_batch_gradient_descent(x_train, y_train, w, b, 0.0000001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.6947610336910675\n",
      "Epoch 100 with loss: 0.8965942654085093\n",
      "Epoch 1000 with loss: 0.8933863075417117\n",
      "Epoch 10000 with loss: 0.8608287841248906\n",
      "Epoch 100000 with loss: 0.5458935141054073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0034322709792326904, 0.1396068685865824, 0.5259463149395227], -0.06333317899438108)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rather than random, try again with zero\n",
    "w, b = train_batch_gradient_descent(x_train, y_train, [0.0, 0.0, 0.0], 0.0, 0.000001, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.5448545091935185\n",
      "Epoch 100 with loss: 0.5430198004193282\n",
      "Epoch 1000 with loss: 0.542985233510204\n",
      "Epoch 10000 with loss: 0.5426421979879512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0032097288355285797, 0.1403572973947398, 0.5285336555309248], -0.06394887120261901)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat training to obtain lower cost, better performancem, continuing the same weights and biase as input\n",
    "w, b = train_batch_gradient_descent(x_train, y_train, w, b, 0.0000001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.5426421601372262\n",
      "Epoch 100 with loss: 0.5426384132044668\n",
      "Epoch 1000 with loss: 0.5426043764098233\n",
      "Epoch 10000 with loss: 0.5422665910412652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0032261062569065844, 0.14110168615558116, 0.5310984938850596], -0.06456476415771177)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep trying, would like a cost of less than 0.5\n",
    "w, b = train_batch_gradient_descent(x_train, y_train, w, b, 0.0000001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.5422665537687784\n",
      "Epoch 100 with loss: 0.5422628640760397\n",
      "Epoch 1000 with loss: 0.5422293471385554\n",
      "Epoch 10000 with loss: 0.5418967105156732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0032423492308344707, 0.1418412316974545, 0.5336423570477462], -0.06518051080196813)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = train_batch_gradient_descent(x_train, y_train, w, b, 0.0000001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.541896673810274\n",
      "Epoch 100 with loss: 0.5418930402537466\n",
      "Epoch 1000 with loss: 0.5418600331503418\n",
      "Epoch 10000 with loss: 0.5415324461770251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0032584592809632956, 0.1425759920874105, 0.5361654810047471], -0.06579611242520751)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = train_batch_gradient_descent(x_train, y_train, w, b, 0.0000001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.5415324100278166\n",
      "Epoch 100 with loss: 0.5415288315288166\n",
      "Epoch 1000 with loss: 0.5414963244645234\n",
      "Epoch 10000 with loss: 0.5411736902961483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.00327443790784152, 0.1433060244684969, 0.5386680982069697], -0.06641157029950655)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = train_batch_gradient_descent(x_train, y_train, w, b, 0.0000001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.5411736546924802\n",
      "Epoch 100 with loss: 0.5411701301967934\n",
      "Epoch 1000 with loss: 0.5411381135988025\n",
      "Epoch 10000 with loss: 0.5408203375835526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.003290286589341863, 0.14403138507717855, 0.5411504376345206], -0.06702688567949266)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = train_batch_gradient_descent(x_train, y_train, w, b, 0.0000001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thats ok if its not less than 0.5, we will now check the amount of error using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    function predict(x, y, w, b)\n",
    "\n",
    "Outputs a prediction of addmitted or not dependent on weights and biases\n",
    "\n",
    "input   : x - feature vector (the GPA, GMAT and work experience of the candidate)\n",
    "input   : labels - correct classification (Accepted or not)\n",
    "input   : w - initial weights for each of the values of x going into the logistic function\n",
    "input   : b - intial bias \n",
    "\n",
    "\n",
    "returns : prints to the screen the prediction and actual label\n",
    "```\n",
    "\"\"\"\n",
    "# function that prints the prediction and the \n",
    "function predict(x, y, w, b)\n",
    "    if σ(w'x+b) >= 0.5\n",
    "        println(\"Predict Accepted\")\n",
    "        y == 1 ? println(\"Was Accepted\") : println(\"[ERROR] Was Not accepted\")\n",
    "    else\n",
    "        println(\"Predict Not Accepted\")\n",
    "        y == 1 ? println(\"[ERROR] Was Accepted\") : println(\"Was Not Accepted\")\n",
    "    end\n",
    "end\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Not Accepted\n",
      "[ERROR] Was Accepted\n",
      "\n",
      "Predict Not Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "[ERROR] Was Not accepted\n",
      "\n",
      "Predict Not Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Not Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Not Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Not Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Not Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Not Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Not Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Not Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "[ERROR] Was Not accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i = 1:length(x_test)\n",
    "    predict(x_test[i], y_test[i], w, b)\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    function predict(x, y, w, b)\n",
    "\n",
    "Outputs a prediction of addmitted or not dependent on weights and biases\n",
    "\n",
    "input   : x - feature vector (the GPA, GMAT and work experience of the candidate)\n",
    "input   : labels - correct classification (Accepted or not)\n",
    "input   : w - initial weights for each of the values of x going into the logistic function\n",
    "input   : b - intial bias \n",
    "\n",
    "\n",
    "returns : 0 or 1, used for numerical computation of error\n",
    "```\n",
    "\"\"\"\n",
    "function predict(x, y, w, b)\n",
    "    if σ(w'x+b) >= 0.5\n",
    "        return 1\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_error"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    function calculate_error(x, y, w, b)\n",
    "\n",
    "Outputs a error amount based on input data and given weights and bias\n",
    "\n",
    "input   : x - feature vector (the GPA, GMAT and work experience of the candidate)\n",
    "input   : y - correct classification (Accepted or not)\n",
    "input   : w - initial weights for each of the values of x going into the logistic function\n",
    "input   : b - intial bias \n",
    "\n",
    "\n",
    "returns : 0 or 1, used for numerical computation of error\n",
    "```\n",
    "\"\"\"\n",
    "function calculate_error(x, y, w, b)\n",
    "    mean_error = 0.0\n",
    "    for i = 1:length(x)\n",
    "        mean_error += (predict(x[i], y[i], w, b) - y[i])^2\n",
    "    end\n",
    "\n",
    "    print(mean_error/length(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15"
     ]
    }
   ],
   "source": [
    "calculate_error(x_test,y_test,w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Using the data and result we can conclude that the model can accurately predict if someone will be admitted based on a logistic combination of their GPA, GMAT and amount of work experience"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
